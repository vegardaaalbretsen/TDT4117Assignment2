{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bb94987",
   "metadata": {},
   "source": [
    "# Assignment 2 : Language Modeling & Evaluation \n",
    "\n",
    "**Student names**: Vegard Aa Albretsen <br>\n",
    "**Group number**: Group 62 <br>\n",
    "**Date**: _We will see_\n",
    "\n",
    "## Important notes\n",
    "Please read and follow these rules. Submissions that do not fulfill them may be returned.\n",
    "1. You may work in groups of maximum 2 students.\n",
    "2. Submit in **.ipynb** format only.\n",
    "3. The assignment must be typed. Handwritten answers are not accepted.\n",
    "\n",
    "**Due date**: 28.09.2025 23:59\n",
    "\n",
    "### What you will do\n",
    "- Build a **unigram document language model** with a **document-term matrix**.\n",
    "- Rank documents for queries using **Jelinek-Mercer smoothing**.\n",
    "- Evaluate the run using **Cranfield queries and qrels** (P@k, MAP, MRR).\n",
    "- (Optional) Try **Dirichlet** and compare briefly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b202a5f0",
   "metadata": {},
   "source": [
    "---\n",
    "## Dataset\n",
    "\n",
    "Make sure the Cranfield files are placed next to the notebook:\n",
    "- `cran.all.1400` — document collection (1400 docs)\n",
    "- `cran.qry` — queries\n",
    "- `cranqrel` — relevance judgments (qrels)\n",
    "\n",
    "> Only the **document parsing** for cran.all.1400 code is provided below. You will implement the rest in the TODO cells.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef4e626",
   "metadata": {},
   "source": [
    "### Load and parse documents (provided)\n",
    "\n",
    "Run the cell to parse the Cranfield documents. Update the path so it points to your `cran.all.1400` file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "84acb800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 1400 documents.\n",
      "[(1, {'id': 1, 'title': 'experimental investigation of the aerodynamics of a wing in a slipstream .', 'abstract': 'experimental investigation of the aerodynamics of a wing in a slipstream .   an experimental study of a wing in a propeller slipstream was made in order to determine the spanwise distribution of the lift increase due to slipstream at different angles of attack of the wing and at different free stream to slipstream velocity ratios .  the results were intended in part as an evaluation basis for different theoretical treatments of this problem .   the comparative span loading curves, together with supporting evidence, showed that a substantial part of the lift increment produced by the slipstream was due to a /destalling/ or boundary-layer-control effect .  the integrated remaining lift increment, after subtracting this destalling lift, was found to agree well with a potential flow theory .   an empirical evaluation of the destalling effects was made for the specific configuration of the experiment .'})]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Read 'cran.all.1400' and parse the documents into a suitable data structure\n",
    "\n",
    "CRAN_PATH = r\"cran.all.1400\"  # <-- change this!\n",
    "\n",
    "def parse_cranfield(path):\n",
    "    docs = {} # docs : Dict[int, Dict[str, Union[int, str]]] -> {id: {id, title, abstract}}\n",
    "    current_id = None\n",
    "    current_field = None\n",
    "    buffers = {\"T\": [], \"A\": [], \"B\": [], \"W\": []}\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for line in f:\n",
    "            line = line.rstrip(\"\\n\")\n",
    "            if line.startswith(\".I \"):\n",
    "                if current_id is not None:\n",
    "                    docs[current_id] = {\n",
    "                        \"id\": current_id,\n",
    "                        \"title\": \" \".join(buffers[\"T\"]).strip(),\n",
    "                        \"abstract\": \" \".join(buffers[\"W\"]).strip()\n",
    "                    }\n",
    "                current_id = int(line.split()[1])\n",
    "                buffers = {k: [] for k in buffers}\n",
    "                current_field = None\n",
    "            elif line.startswith(\".\"):\n",
    "                tag = line[1:].strip()\n",
    "                current_field = tag if tag in buffers else None\n",
    "            else:\n",
    "                if current_field is not None:\n",
    "                    buffers[current_field].append(line)\n",
    "    if current_id is not None:\n",
    "        docs[current_id] = {\n",
    "            \"id\": current_id,\n",
    "            \"title\": \" \".join(buffers[\"T\"]).strip(),\n",
    "            \"abstract\": \" \".join(buffers[\"W\"]).strip()\n",
    "        }\n",
    "    print(f\"Parsed {len(docs)} documents.\")\n",
    "    return docs\n",
    "\n",
    "docs = parse_cranfield(CRAN_PATH)\n",
    "print(list(docs.items())[:1])  # peek at the first parsed doc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03aa91cc",
   "metadata": {},
   "source": [
    "## 2.1 Language Modeling\n",
    "\n",
    "You will create a **unigram language model** per document, using a **document-term matrix**, and score queries with **Jelinek-Mercer smoothing**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ade37a",
   "metadata": {},
   "source": [
    "### 2.1.1 Preprocessing\n",
    "\n",
    "Implement a simple tokenizer/normalizer (e.g., lowercasing, punctuation removal and stopword removal) and apply it to each document\n",
    "\n",
    "- Return a list of tokens for each document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "abd9b2cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example document tokens: ['experimental', 'investigation', 'aerodynamics', 'wing', 'slipstream', 'experimental', 'investigation', 'aerodynamics', 'wing', 'slipstream']\n",
      "Document 1 has 84 tokens\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement preprocessing and apply to all documents\n",
    "\n",
    "STOPWORDS = set(\"\"\"a about above after again against all am an and any are aren't as at be because been\n",
    "before being below between both but by can't cannot could couldn't did didn't do does doesn't doing don't down\n",
    "during each few for from further had hadn't has hasn't have haven't having he he'd he'll he's her here here's hers\n",
    "herself him himself his how how's i i'd i'll i'm i've if in into is isn't it it's its itself let's me more most\n",
    "mustn't my myself no nor not of off on once only or other ought our ours ourselves out over own same shan't she\n",
    "she'd she'll she's should shouldn't so some such than that that's the their theirs them themselves then there there's\n",
    "these they they'd they'll they're they've this those through to too under until up very was wasn't we we'd we'll we're\n",
    "we've were weren't what what's when when's where where's which while who who's whom why why's with won't would wouldn't\n",
    "you you'd you'll you're you've your yours yourself yourselves\"\"\".split())\n",
    "\n",
    "import re\n",
    "\n",
    "def preprocess(text): \n",
    "    text = text.lower()\n",
    "    tokens = re.findall(r'\\b[a-zA-Z]+\\b', text)\n",
    "    tokens = [token for token in tokens if token not in STOPWORDS]\n",
    "    return tokens\n",
    "\n",
    "for doc_id, doc_data in docs.items():\n",
    "    combined_text = doc_data['title'] + \" \" + doc_data['abstract']\n",
    "    doc_data['tokens'] = preprocess(combined_text)\n",
    "\n",
    "\n",
    "print(f\"Example document tokens: {list(docs.items())[0][1]['tokens'][:10]}\")\n",
    "print(f\"Document 1 has {len(docs[1]['tokens'])} tokens\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b959a1",
   "metadata": {},
   "source": [
    "### 2.1.2 Build the matrix\n",
    "\n",
    "Construct:\n",
    "- A vocabulary `term -> column_index`\n",
    "- A (sparse) **document–term count matrix**\n",
    "- Document lengths `|d|` and collection (all documents) totals\n",
    "\n",
    "> Tip: You may use dictionaries or `scipy.sparse` for efficiency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d74cdc73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 6928\n",
      "Total documents: 1400\n",
      "Document 1 length: 84\n",
      "Total collection tokens: 140876\n"
     ]
    }
   ],
   "source": [
    "# TODO: Build vocabulary, doc-term counts (sparse), document lengths, and collection totals\n",
    "from collections import defaultdict\n",
    "vocabulary = set()\n",
    "doc_term_counts = {'term':str,'count':int}\n",
    "for doc_id, doc_data in docs.items():\n",
    "    for token in doc_data['tokens']:\n",
    "        vocabulary.add(token)\n",
    "\n",
    "vocab_to_idx = {term: idx for idx, term in enumerate(vocabulary)}\n",
    "\n",
    "doc_term_counts = {}\n",
    "doc_lengths = {}\n",
    "collection_totals = defaultdict(int)\n",
    "\n",
    "for doc_id, doc_data in docs.items():\n",
    "    term_counts = defaultdict(int)\n",
    "    for token in doc_data['tokens']:\n",
    "        term_counts[token] += 1\n",
    "        collection_totals[token] +=1\n",
    "    \n",
    "    doc_term_counts[doc_id] = dict(term_counts)\n",
    "    doc_lengths[doc_id] = len(doc_data['tokens'])\n",
    "\n",
    "print(f\"Vocabulary size: {len(vocabulary)}\")\n",
    "print(f\"Total documents: {len(doc_term_counts)}\")\n",
    "print(f\"Document 1 length: {doc_lengths[1]}\")\n",
    "print(f\"Total collection tokens: {sum(collection_totals.values())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0affe47",
   "metadata": {},
   "source": [
    "### 2.1.3 Rank with **Jelinek-Mercer smoothing**\n",
    "\n",
    "Implement query likelihood scoring with Jelinek-Mercer smoothing:\n",
    "\n",
    "$\\hat{P}(t \\mid M_d) = \\lambda \\hat{P}_{\\text{mle}}(t \\mid M_d) + (1 - \\lambda)\\hat{P}_{\\text{mle}}(t \\mid M_c), \\ \\lambda = 0.5$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147c30cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" TODO: Create a function for implementing query likelihood scoring with Jelinek Mercer smoothing (λ=0.5), \n",
    "using the formula above.\"\"\"\n",
    "\n",
    "def jelinek_smoothing(query: str):\n",
    "    # 1. Setup\n",
    "    # Set smoothing parameter to 0.5\n",
    "    # calculate total tokens in entire collection\n",
    "    smooth_par = 0.5\n",
    "    # Total tokens: collection_totals.values()\n",
    "\n",
    "    # 2. Query processing\n",
    "    # tokenize and preprocess query\n",
    "    # extract individual query terms\n",
    "    query_tokens = preprocess(query)\n",
    "    \n",
    "    # 3. Document scoring loop\n",
    "    document_scores = {}\n",
    "    # For each document, init document score to zero\n",
    "    for doc_id, doc_data in docs.items():\n",
    "        document_scores.update({\"id\":doc_id,\"scores\":0})\n",
    "\n",
    "    # 4. Term probability calculation\n",
    "    # for each term\n",
    "    # get term frequency\n",
    "    # calculate document probability (term freq / doc length)\n",
    "    # calculate collection probability (term freq / collection size)\n",
    "\n",
    "    # 5. Apply smoothing formula\n",
    "    # combine document and collection probabilities using smoothing parameter\n",
    "    # use logarithms to prevent numerical underflow\n",
    "    # add term score to documents total score\n",
    "\n",
    "    # 6. Ranking\n",
    "    # sort all documents by their total scores (highest first)\n",
    "    # add return ranked list of document IDs\n",
    "    raise NotImplementedError(\"Implement Jelinek-Mercer smoothing here\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e389f7fe",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Do not change this code\n",
    "queries_assignment2 = [\n",
    "  \"gas pressure\",\n",
    "  \"structural aeroelastic flight high speed aircraft\",\n",
    "  \"heat conduction composite slabs\",\n",
    "  \"boundary layer control\",\n",
    "  \"compressible flow nozzle\",\n",
    "  \"combustion chamber injection\",\n",
    "  \"laminar turbulent transition\",\n",
    "  \"fatigue crack growth\",\n",
    "  \"wing tip vortices\",\n",
    "  \"propulsion efficiency\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b153b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Jelinek-Mercer smoothing on queries in batch (print top-10 results for each), using the function you created\n",
    "def run_batch_smoothing(queries):\n",
    "    results = {}\n",
    "    for i, q in enumerate(queries, 1):\n",
    "        res = jelinek_smoothing(q)\n",
    "        results[f\"Q{i}\"] = res\n",
    "    return results\n",
    "\n",
    "jelinek_results = run_batch_smoothing(queries_assignment2)\n",
    "\n",
    "for qid, res in jelinek_results.items():\n",
    "    print(qid, \"=>\", res[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6157d304",
   "metadata": {},
   "source": [
    "#### Dirichlet\n",
    "\n",
    "If you have time, also implement Dirichlet smoothing and briefly compare the top-10 lists for the first query in the queries_assignment2 list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fed468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (Optional): Implement Dirichlet scoring and compare with Jelinek-Mercer\n",
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0657d9",
   "metadata": {},
   "source": [
    "## 2.2 Evaluation (Cranfield queries + qrels)\n",
    "\n",
    "Evaluate your retrieval system using **Cranfield**:\n",
    "- Parse **queries** from `cran.qry`\n",
    "- Parse **relevance judgments** from `cranqrel`\n",
    "- Compute **P@k (k=5,10)**, **MAP (Mean Average Precision)**, and **MRR (Mean Reciprocal Rank)** over all queries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80204da4",
   "metadata": {},
   "source": [
    "### 2.2.1 Parse `cran.qry` and `cranqrel`\n",
    "\n",
    "- Create `queries[qid] = \"text\"` by parsing `cran.qry`\n",
    "- Create `qrels[qid] = set(relevant_doc_ids)` by parsing `cranqrel`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a91e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Parse cran.qry and cranqrel into convenient data structures\n",
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de736084",
   "metadata": {},
   "source": [
    "### 2.2.2 Implement metrics: P@k, MAP, MRR\n",
    "\n",
    "Write functions to compute:\n",
    "- Precision@k (for **k=5** and **k=10**)\n",
    "- Mean Average Precision (MAP)\n",
    "- Mean Reciprocal Rank (MRR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258d08c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement P@k (k=5,10), MAP, and MRR\n",
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3b07a1",
   "metadata": {},
   "source": [
    "### 2.2.3 Evaluate your run\n",
    "\n",
    "- For **all queries**, generate rankings with your **Jelinek-Mercer** model\n",
    "- Report aggregate metrics: P@5, P@10, MAP, MRR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f79ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Run retrieval for all queries and compute P@5, P@10, MAP, and MRR\n",
    "# Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e466be",
   "metadata": {},
   "source": [
    "\n",
    "### 2.2.4 Interpolated precision–recall curves (11‑point)\n",
    "\n",
    "- For **all queries**, if you don’t have query IDs, assign sequential IDs: `Q1, Q2, ..., Qm` in the order they appear.\n",
    "- Using your **rankings from task 2.2.3** and the **relevance judgments (`cranqrel`)**, compute **precision** and **recall** at each rank for each query.\n",
    "- For the 11 standard recall levels `R = {0.0, 0.1, ..., 1.0}`, compute the **interpolated precision** at level `r` as the **maximum precision** observed at any point with recall ≥ `r`.\n",
    "- **Report/plot** the **11‑point interpolated precision–recall curve** across queries (and optionally a few per‑query curves)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1721ea9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Compute the 11-point interpolated precision–recall for each query\n",
    "\n",
    "# Your code here\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
